{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DRBqUUHoAD5J","executionInfo":{"status":"ok","timestamp":1719215244039,"user_tz":-330,"elapsed":22581,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}},"outputId":"7fa0572b-ae39-4050-f553-e4a56ae6f5dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"JT3Cs0u7i89S"},"source":["vgg pretrained - not fine tuned (multi - channel)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u03IQDMNgVTw"},"outputs":[],"source":["import tensorflow as tf\n","from keras.layers import Conv3D, Dense, MaxPooling3D, Flatten, Dropout, BatchNormalization, LeakyReLU\n","from tensorflow.keras.layers import Input, GlobalAveragePooling2D, MaxPooling2D, Conv2D\n","from keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from keras.losses import binary_crossentropy, categorical_crossentropy\n","from keras.applications.vgg16 import VGG16\n","\n","from keras.applications.vgg16 import preprocess_input\n","\n","import numpy as np\n","\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, f1_score, recall_score, confusion_matrix\n","from sklearn.metrics import roc_curve, auc\n","\n","import os\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","from matplotlib import pyplot as plt\n","\n","import csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"claRr50Ggsp0"},"outputs":[],"source":["# adam optimizer default learning rate = 0.001\n","lr = 0.0001 #learning rate\n","ep = 1\n","batch_size = 1\n","n_splits = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_HH3CJSXgskq"},"outputs":[],"source":["# not performing fine tuning during training\n","def get_model(input_shape, num_classes):\n","\n","    #returns compiled model for the smri pipeline\n","    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape) #(224, 224, 3)\n","\n","    # Add classification layers\n","    x = base_model.output\n","    x = Flatten()(x)\n","    x = Dense(512, activation='relu')(x)\n","    x = Dropout(0.5)(x)\n","    predictions = Dense(num_classes, activation='softmax')(x)\n","\n","    model = Model(inputs=base_model.input, outputs=predictions)\n","\n","    for layer in base_model.layers:\n","        layer.trainable = False\n","\n","    model.compile(optimizer=Adam(learning_rate = lr), loss='categorical_crossentropy', metrics=['accuracy'])\n","    print(\"Output shape of the model:\", model.output_shape)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tvCCw0z4gsiS"},"outputs":[],"source":["def compile_fit(model, X_train, y_train, X_val, y_val):\n","\n","    # print(type(X_train), type(X_val))\n","    # print(type(y_train), type(y_val))\n","\n","    hist = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=ep, batch_size=batch_size)\n","\n","    return model, hist"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HmRBDNxVgsfy"},"outputs":[],"source":["def plot_history(hist, n):\n","    fig, ax = plt.subplots(2)\n","    ax[0].plot(hist.history['accuracy'], color=\"blue\")\n","    ax[0].plot(hist.history['val_accuracy'], color=\"green\")\n","    ax[0].set(xlabel=\"epochs\", ylabel=\"Accuracy\")\n","    ax[0].set_xlim((0,ep))\n","    ax[0].set_ylim((0,1))\n","    ax[0].legend(['training', 'testing'])\n","    ax[0].set_title(\"Accuracy Trend\")\n","\n","    ax[1].plot(hist.history['loss'], color=\"blue\")\n","    ax[1].plot(hist.history['val_loss'], color=\"green\")\n","    ax[1].set(xlabel=\"epochs\", ylabel=\"Loss\")\n","    ax[1].set_xlim((0,ep))\n","    ax[1].set_ylim((0,1))\n","    ax[1].legend(['training', 'testing'])\n","    ax[1].set_title(\"Loss Trend\")\n","\n","    fig.tight_layout(pad=2.0)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BULz6h_HjOe8"},"outputs":[],"source":["def eval_model(num_classes, model, X_val, y_val, y_cols):\n","  # Compute loss and accuracy using model.evaluate()\n","  loss, acc = model.evaluate(X_val, y_val)\n","\n","  y_pred = model.predict(X_val)\n","  print('y_pred', y_pred)\n","  print('y_val', y_val)\n","\n","  if (num_classes == 3) :\n","\n","    # Convert y_val to multiclass format\n","    y_val = np.argmax(y_val, axis=1)\n","\n","    # Convert y_pred to multiclass format\n","    y_pred = np.argmax(y_pred, axis=1)\n","\n","    print('y_pred', y_pred)\n","    print('y_val', y_val)\n","\n","    # Check the type of y_pred_probs\n","    print(\"Type of y_pred using model.predict:\", type(y_pred))\n","    print(\"shape of the y_pred using model.predict:\", y_pred.shape)\n","\n","    # Compute confusion matrix\n","    # y_val_argmax = np.argmax(y_val, axis=1)\n","    # y_pred_argmax = np.argmax(y_pred, axis=1)\n","    # conf_mat = confusion_matrix(y_val_argmax, y_pred_argmax)\n","\n","    conf_mat = mt.confusion_matrix(y_val, y_pred)\n","    print(\"confusion matrix \", conf_mat)\n","\n","    target_names = y_cols\n","\n","    print(\"classification report\", mt.classification_report(y_val, y_pred, target_names=target_names, digits = 3))\n","\n","    # Compute classification report\n","    report = mt.classification_report(y_val, y_pred, target_names=target_names, output_dict=True)\n","    report_df = pd.DataFrame(report).T\n","\n","    print(\"classification report in dataframe - match accuracy with model.evaluate \")\n","    print(report_df)\n","\n","    # Select the first three rows\n","    report_df_top3 = report_df.head(3)\n","\n","    # Calculate average metrics for the first three rows\n","    avg_precision = report_df_top3['precision'].mean()\n","    avg_recall = report_df_top3['recall'].mean()\n","    avg_f1_score = report_df_top3['f1-score'].mean()\n","\n","    print(f\"Average Precision (first 3 classes): {avg_precision:.3f}\")\n","    print(f\"Average Recall (first 3 classes): {avg_recall:.3f}\")\n","    print(f\"Average F1-Score (first 3 classes): {avg_f1_score:.3f}\")\n","\n","    metrics = {\n","        'acc': acc,\n","        'loss': loss,\n","        'conf_mat': conf_mat,\n","        'sens (recall)': avg_recall,\n","        'f1': avg_f1_score,\n","        'prec': avg_precision\n","    }\n","\n","  elif (num_classes == 2) :\n","\n","    # Convert y_val to multiclass format\n","    y_val = np.argmax(y_val, axis=1)\n","\n","    # Convert y_pred to multiclass format\n","    y_pred = np.argmax(y_pred, axis=1)\n","\n","    print('y_pred', y_pred)\n","    print('y_val', y_val)\n","\n","    # Check the type of y_pred_probs\n","    print(\"Type of y_pred using model.predict:\", type(y_pred))\n","    print(\"shape of the y_pred using model.predict:\", y_pred.shape)\n","\n","    # Compute confusion matrix\n","    conf_mat = confusion_matrix(y_val, y_pred)\n","\n","    # Compute confusion matrix\n","    # y_pred = np.argmax(y_pred, axis=1)\n","    # conf_mat = confusion_matrix(y_val, y_pred)\n","\n","    # Compute metrics from confusion matrix\n","    tn, fp, fn, tp = conf_mat.ravel()\n","    precision = precision_score(y_val, y_pred)\n","    recall = recall_score(y_val, y_pred)\n","    f1 = f1_score(y_val, y_pred)\n","\n","    metrics = {\n","        'acc': acc,\n","        'loss': loss,\n","        'conf_mat': conf_mat,\n","        'sens (recall)': recall,\n","        'f1': f1,\n","        'prec': precision,\n","        'tn': tn,\n","        'tp': tp,\n","        'fn': fn,\n","        'fp': fp\n","    }\n","\n","  else :\n","    metrics = {}\n","\n","  return metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNvEBOc_IUEv","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1719215260844,"user_tz":-330,"elapsed":25,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}},"outputId":"6f283af0-254c-46ac-f3f8-b4a285065148"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n# Replacing nan values to 1\\ndef nan_to_0(data):\\n\\n    df1 = data.copy()\\n\\n    for idx, row in df1.iterrows():\\n      arr = row['fcmap']\\n      matrix = np.nan_to_num(arr, copy = True, nan = 0.0)\\n      df1.at[idx, 'fcmap'] = matrix\\n\\n    print(df1)\\n\\n    return df\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}],"source":["'''\n","# Replacing nan values to 1\n","def nan_to_0(data):\n","\n","    df1 = data.copy()\n","\n","    for idx, row in df1.iterrows():\n","      arr = row['fcmap']\n","      matrix = np.nan_to_num(arr, copy = True, nan = 0.0)\n","      df1.at[idx, 'fcmap'] = matrix\n","\n","    print(df1)\n","\n","    return df\n","'''"]},{"cell_type":"code","source":["'''\n","def computeMinMax(X):\n","  min_matrix = X.min(axis = 0)\n","  max_matrix = X.max(axis = 0)\n","  return (min_matrix, max_matrix)\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"tY0E61AuB5nX","executionInfo":{"status":"ok","timestamp":1719215260844,"user_tz":-330,"elapsed":12,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}},"outputId":"78990373-346b-430f-f525-dafecc1c0f66"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef computeMinMax(X):\\n  min_matrix = X.min(axis = 0)\\n  max_matrix = X.max(axis = 0)\\n  return (min_matrix, max_matrix)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["'''\n","def normalize_instance(X, minn, maxx):\n","  normalised_X = np.zeros(shape=(X.shape[0], X.shape[1]))\n","\n","  for idx, x in np.ndenumerate(X):\n","    if minn[idx] == maxx[idx]:\n","      normalised_X[idx] = x\n","    else:\n","      normalised_X[idx] = (x - minn[idx])/(maxx[idx] - minn[idx])\n","    return normalised_X\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"36JJA2vWB5c3","executionInfo":{"status":"ok","timestamp":1719215260844,"user_tz":-330,"elapsed":12,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}},"outputId":"e64400c7-01ce-4367-8710-3693a033ebe8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef normalize_instance(X, minn, maxx):\\n  normalised_X = np.zeros(shape=(X.shape[0], X.shape[1]))\\n\\n  for idx, x in np.ndenumerate(X):\\n    if minn[idx] == maxx[idx]:\\n      normalised_X[idx] = x\\n    else:\\n      normalised_X[idx] = (x - minn[idx])/(maxx[idx] - minn[idx])\\n    return normalised_X\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["'''\n","def normalize(X_train, X_val):\n","    # Assuming X_train is your DataFrame with matrices in a single column\n","    matrices = X_train  # Get the values from the 'matrices' column\n","    # Convert the matrices to a 2D NumPy array\n","    X_train_2d = np.stack(matrices)\n","\n","    # Assuming X_train is your DataFrame with matrices in a single column\n","    matrices = X_val  # Get the values from the 'matrices' column\n","    # Convert the matrices to a 2D NumPy array\n","    X_val_2d = np.stack(matrices)\n","\n","    min_matrix, max_matrix = computeMinMax(X_train_2d)\n","\n","    print(\"shape of min matrix\", min_matrix.shape)\n","    print(\"shape of max matrix\", max_matrix.shape)\n","\n","    normalized_instances = []\n","    for instance in X_train_2d:\n","        normalized_instance = normalize_instance(instance, min_matrix, max_matrix)\n","        normalized_instances.append(normalized_instance)\n","\n","    # Convert the list of normalized instances to a NumPy array\n","    X_normalized_trained_2d = np.array(normalized_instances)\n","\n","    normalized_instances = []\n","    for instance in X_val_2d:\n","        normalized_instance = normalize_instance(instance, min_matrix, max_matrix)\n","        normalized_instances.append(normalized_instance)\n","\n","    # Convert the list of normalized instances to a NumPy array\n","    X_normalized_val_2d = np.array(normalized_instances)\n","\n","    return (X_normalized_trained_2d, X_normalized_val_2d)\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":157},"id":"elWLZ7I1B5Z4","executionInfo":{"status":"ok","timestamp":1719215260844,"user_tz":-330,"elapsed":11,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}},"outputId":"305cb1ea-4c2f-4b05-cc4d-163eb2993fce"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef normalize(X_train, X_val):\\n    # Assuming X_train is your DataFrame with matrices in a single column\\n    matrices = X_train  # Get the values from the \\'matrices\\' column\\n    # Convert the matrices to a 2D NumPy array\\n    X_train_2d = np.stack(matrices)\\n\\n    # Assuming X_train is your DataFrame with matrices in a single column\\n    matrices = X_val  # Get the values from the \\'matrices\\' column\\n    # Convert the matrices to a 2D NumPy array\\n    X_val_2d = np.stack(matrices)\\n\\n    min_matrix, max_matrix = computeMinMax(X_train_2d)\\n\\n    print(\"shape of min matrix\", min_matrix.shape)\\n    print(\"shape of max matrix\", max_matrix.shape)\\n\\n    normalized_instances = []\\n    for instance in X_train_2d:\\n        normalized_instance = normalize_instance(instance, min_matrix, max_matrix)\\n        normalized_instances.append(normalized_instance)\\n\\n    # Convert the list of normalized instances to a NumPy array\\n    X_normalized_trained_2d = np.array(normalized_instances)\\n\\n    normalized_instances = []\\n    for instance in X_val_2d:\\n        normalized_instance = normalize_instance(instance, min_matrix, max_matrix)\\n        normalized_instances.append(normalized_instance)\\n\\n    # Convert the list of normalized instances to a NumPy array\\n    X_normalized_val_2d = np.array(normalized_instances)\\n\\n    return (X_normalized_trained_2d, X_normalized_val_2d)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gvFdoN0ILlTL"},"outputs":[],"source":["# Padding Matrices\n","# def padding_data(data):\n","#     def pad_matrix(row):\n","#         arr = row['fcmap']\n","#         pad_top = (224 - arr.shape[0]) // 2\n","#         pad_bottom = 224 - arr.shape[0] - pad_top\n","#         pad_left = (224 - arr.shape[1]) // 2\n","#         pad_right = 224 - arr.shape[1] - pad_left\n","#         padded_matrix = np.pad(arr, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant')\n","#         return padded_matrix\n","\n","#     df_pad = data.copy()\n","\n","#     df_pad['fcmap'] = df_pad.apply(lambda row: pad_matrix(row), axis=1)\n","\n","#     return df_pad\n","\n","def padding_data(X, target_shape=(224, 224)):\n","    \"\"\"\n","    Pad each matrix in X_train to the target shape.\n","\n","    Args:\n","        X_train (numpy.ndarray): Input array containing matrices.\n","        target_shape (tuple): Target shape for padding.\n","\n","    Returns:\n","        numpy.ndarray: Padded matrices.\n","    \"\"\"\n","    padded_matrices = []\n","\n","    for matrix in X:\n","        pad_top = (target_shape[0] - matrix.shape[0]) // 2\n","        pad_bottom = target_shape[0] - matrix.shape[0] - pad_top\n","        pad_left = (target_shape[1] - matrix.shape[1]) // 2\n","        pad_right = target_shape[1] - matrix.shape[1] - pad_left\n","\n","        padded_matrix = np.pad(matrix, ((pad_top, pad_bottom), (pad_left, pad_right), (0, 0)), mode='constant')\n","        padded_matrices.append(padded_matrix)\n","\n","    return np.array(padded_matrices)"]},{"cell_type":"code","source":["'''\n","def increase_channels(X):\n","    # preparing data for vgg pre-trained\n","    X = np.stack(X).reshape(-1, 132, 132)\n","    print(\"stacking all matrices together\", X.shape)\n","    X = np.expand_dims(X, axis=-1)\n","    print(\"adding channel dimension\", X.shape)\n","    # X = np.repeat(X, 3, axis=-1)\n","    # print(\"increasing channels for vgg\", X.shape)\n","\n","    return X\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"1fWhcrgKCLe_","executionInfo":{"status":"ok","timestamp":1719215260845,"user_tz":-330,"elapsed":11,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}},"outputId":"9f848b60-4d98-4c49-ab98-f3cd662f4db1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef increase_channels(X):\\n    # preparing data for vgg pre-trained\\n    X = np.stack(X).reshape(-1, 132, 132)\\n    print(\"stacking all matrices together\", X.shape)\\n    X = np.expand_dims(X, axis=-1)\\n    print(\"adding channel dimension\", X.shape)\\n    # X = np.repeat(X, 3, axis=-1)\\n    # print(\"increasing channels for vgg\", X.shape)\\n\\n    return X\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WmJP3YLbjOca"},"outputs":[],"source":["def make_dataset(choice):\n","    # Load dataframe from the pickle file\n","    data = pd.read_pickle('/content/drive/MyDrive/Colab Notebooks/ROIxTimeseries/fcmap+psi+pearson_data.pkl')\n","\n","    if choice == 'A':\n","        # Filter rows where 'adhd' or 'autism' is 1 (keep only ADHD or autism subjects)\n","        data = data[(data['adhd'] == 1) | (data['autism'] == 1)]\n","        y_cols = ['adhd', 'autism']  # Specify the columns for y\n","    elif choice == 'B':\n","        # Filter rows where 'autism' or 'healthy' is 1 (keep only autism or healthy subjects)\n","        data = data[(data['autism'] == 1) | (data['healthy'] == 1)]\n","        y_cols = ['autism', 'healthy']  # Specify the columns for y\n","    elif choice == 'C':\n","        # Filter rows where 'adhd' or 'healthy' is 1 (keep only ADHD or healthy subjects)\n","        data = data[(data['adhd'] == 1) | (data['healthy'] == 1)]\n","        y_cols = ['adhd', 'healthy']  # Specify the columns for y\n","    elif choice == 'D':\n","        # Keep all rows\n","        y_cols = ['adhd', 'autism', 'healthy']  # Specify the columns for y\n","    else:\n","        print(\"Invalid choice. Please enter 'A', 'B', 'C', or 'D'.\")\n","        return pd.DataFrame(), []\n","\n","    print(data)\n","\n","    # df1 = nan_to_0(data)\n","\n","    print(y_cols)\n","    return data, y_cols"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M3mrlUZmjOaS"},"outputs":[],"source":["def driver(choice):\n","\n","    # choice = input(\"Enter your choice (A, B, C, or D): \").upper()\n","\n","    choice = choice.upper()\n","\n","    df, y_cols = make_dataset(choice)\n","    #print(d.head)\n","\n","    X = df['combined_matrix']\n","    print(X.shape)\n","    print(\"type of matrices\", type(X))\n","\n","    X = padding_data(X)\n","    print(\"after padding\",X.shape)\n","    print(\"type of matrices after padding\", type(X))\n","\n","    y = df[y_cols].values\n","    #y = to_categorical(y, num_classes=3)\n","    # print(y.shape)\n","    # print(y)\n","    print(\"type of label columns\", type(y))\n","\n","    # Get the number of classes\n","    num_classes = y.shape[1]\n","    print(\"No. of classes\", num_classes)\n","\n","    input_shape = X[0].shape\n","    print(\"Input_shape:\", input_shape)\n","\n","    if (num_classes == 2) :\n","      result_df = pd.DataFrame(columns = ['fold','acc','loss','conf_mat', 'sens (recall)','f1','prec', 'tn', 'tp', 'fn', 'fp'])\n","    elif (num_classes == 3) :\n","      result_df = pd.DataFrame(columns = ['fold','acc','loss','conf_mat', 'sens (recall)','f1','prec'])\n","    else :\n","      result_df = {}\n","\n","    # n_splits = n_splits # Number of folds\n","\n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","\n","    tf.keras.backend.clear_session()\n","    for i, (train_index, val_index) in enumerate(kf.split(X, y)):\n","\n","        print(\"FOLD : \", i+1)\n","\n","        X_train, X_val = X[train_index], X[val_index]\n","        y_train, y_val = y[train_index], y[val_index]\n","\n","        # Iterate over the matrices in X_train and convert them to tensors\n","        X_train_tensors = tf.convert_to_tensor([tf.convert_to_tensor(matrix.astype(np.float32)) for matrix in X_train])\n","        X_val_tensors = tf.convert_to_tensor([tf.convert_to_tensor(matrix.astype(np.float32)) for matrix in X_val])\n","        # Convert y_train and y_val to tensors\n","        y_train_tensors = tf.convert_to_tensor(y_train.astype(np.float32))\n","        y_val_tensors = tf.convert_to_tensor(y_val.astype(np.float32))\n","\n","        # X_normal_train, X_normal_val = normalize(X_train, X_val)\n","\n","        # X_train_1 = increase_channels(X_normal_train)\n","        # X_val_1 = increase_channels(X_normal_val)\n","\n","        # input_shape_2 =  X_train_1[0].shape\n","        # print(\"Input_shape with channel: \", input_shape_2) # in case, sent as argument to get_model()\n","\n","        # compiled_m = get_model(num_classes)\n","        compiled_m = get_model(input_shape,num_classes)\n","\n","        trained_m, history = compile_fit(compiled_m, X_train_tensors, y_train_tensors, X_val_tensors, y_val_tensors)\n","        plot_history(history, i+1)\n","\n","        scores = eval_model(num_classes, trained_m, X_val_tensors, y_val_tensors, y_cols)\n","        scores['fold']=i+1\n","        print(\"Scores\", scores)\n","        scores = pd.DataFrame([scores])\n","        result_df = pd.concat([result_df,scores], ignore_index=True)\n","        tf.keras.backend.clear_session()\n","\n","    return result_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OsM8tOgljOYL","outputId":"0c0e9f7c-e4c6-445c-8492-60066fbe20d3","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["        subject                                    combined_matrix autism  \\\n","0      subject1  [[[0.0, 1.0, 0.0], [0.9769444491181637, 0.8016...      1   \n","1      subject2  [[[0.0, 1.0, 0.0], [0.9935199866799433, 0.9101...      1   \n","2      subject3  [[[0.0, 1.0, 0.0], [0.992569280979119, 0.75301...      1   \n","3      subject4  [[[0.0, 1.0, 0.0], [0.9984916334274369, 0.8178...      1   \n","4      subject5  [[[0.0, 1.0, 0.0], [0.9920728128274142, 0.7477...      1   \n","..          ...                                                ...    ...   \n","105  subject106  [[[0.0, 1.0, 0.0], [0.7935194794747894, 0.7190...      0   \n","106  subject107  [[[0.0, 1.0, 0.0], [0.8799582823263669, 0.8923...      0   \n","107  subject108  [[[0.0, 1.0, 0.0], [0.9508848131818459, 0.9383...      0   \n","108  subject109  [[[0.0, 1.0, 0.0], [0.9414804729782632, 0.9300...      0   \n","109  subject110  [[[0.0, 1.0, 0.0], [0.8507568146625619, 0.7946...      0   \n","\n","    adhd healthy  \n","0      0       0  \n","1      0       0  \n","2      0       0  \n","3      0       0  \n","4      0       0  \n","..   ...     ...  \n","105    1       0  \n","106    1       0  \n","107    1       0  \n","108    1       0  \n","109    1       0  \n","\n","[110 rows x 5 columns]\n","['adhd', 'autism']\n","(110,)\n","type of matrices <class 'pandas.core.series.Series'>\n","after padding (110, 224, 224, 3)\n","type of matrices after padding <class 'numpy.ndarray'>\n","type of label columns <class 'numpy.ndarray'>\n","No. of classes 2\n","Input_shape: (224, 224, 3)\n","FOLD :  1\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58889256/58889256 [==============================] - 0s 0us/step\n","Output shape of the model: (None, 2)\n","21/88 [======>.......................] - ETA: 2:54 - loss: 1.5909 - accuracy: 0.5714"]}],"source":["# Define a list of choices\n","# choices = ['A', 'B', 'C', 'D']\n","choices = ['A']\n","\n","# Create an empty dictionary to store the result dataframes\n","result_dfs = {}\n","\n","# Loop through each choice\n","for choice in choices:\n","    # Call the driver() function with the current choice\n","    result_df = driver(choice)\n","\n","    # Store the result dataframe in the dictionary with the choice as the key\n","    result_dfs[choice] = result_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wq2XqcnrjOVq"},"outputs":[],"source":["print(result_dfs['A'])"]},{"cell_type":"code","source":["print(result_dfs['B'])"],"metadata":{"id":"wigdPDw6Cfxw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(result_dfs['C'])"],"metadata":{"id":"M10_ByT3CfuY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(result_dfs['D'])"],"metadata":{"id":"KUM42SEECfq4"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"17nltaytO2jhV417rAZFatWxrao7kLyeV","timestamp":1719215582728},{"file_id":"1sZ38Vhj4MVdNttIZn_ya6pwSPrmQCBBD","timestamp":1719214422420}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":0}