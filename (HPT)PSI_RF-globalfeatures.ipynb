{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkMi7V0hu3FG","outputId":"55657d0c-e577-4954-dd7d-843df20a19fe","executionInfo":{"status":"ok","timestamp":1722479333692,"user_tz":-330,"elapsed":35191,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"pCQDBcdhiYbS"},"source":["PSI Raw global features - RF"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"u03IQDMNgVTw","executionInfo":{"status":"ok","timestamp":1722482428649,"user_tz":-330,"elapsed":679,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[],"source":["import numpy as np\n","import networkx as nx\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, f1_score, recall_score, confusion_matrix\n","from sklearn.metrics import roc_curve, auc\n","from sklearn import metrics as mt\n","\n","import os\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","from matplotlib import pyplot as plt\n","\n","import csv"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"EUv62ivEt62F","executionInfo":{"status":"ok","timestamp":1722482429295,"user_tz":-330,"elapsed":2,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[],"source":["n_splits = 5"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"_HH3CJSXgskq","executionInfo":{"status":"ok","timestamp":1722482429295,"user_tz":-330,"elapsed":2,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[],"source":["def get_model(num_classes):\n","    \"\"\"\n","    Creates a Random Forest model for classification.\n","\n","    Args:\n","        num_classes (int): Number of classes in the target variable.\n","\n","    Returns:\n","        RandomForestClassifier: A Random Forest model instance.\n","    \"\"\"\n","\n","    if num_classes == 2:\n","        # Binary classification\n","        rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n","    else:\n","        # Multi-class classification\n","        rf = RandomForestClassifier(random_state=42, class_weight='balanced_subsample')\n","\n","    # Define the parameter grid\n","    param_grid = {\n","        'random_state': [12, 42, 58, 76, 92],\n","        'n_estimators': [50, 100, 150, 200, 250],\n","        'max_depth': [None, 10, 20, 25, 30]\n","    }\n","\n","    # Create GridSearchCV object\n","    grid_search = GridSearchCV(\n","        estimator=rf,\n","        param_grid=param_grid,\n","        cv=3,  # Use 3-fold cross-validation within the grid search\n","        scoring='f1_weighted',  # Use weighted F1-score for multi-class\n","        n_jobs=-1  # Use all available CPU cores\n","    )\n","\n","    return grid_search"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"tvCCw0z4gsiS","executionInfo":{"status":"ok","timestamp":1722482429295,"user_tz":-330,"elapsed":2,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[],"source":["def compile_fit(model, X_train, y_train):\n","    model.fit(X_train, y_train)\n","    return model"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"BULz6h_HjOe8","executionInfo":{"status":"ok","timestamp":1722482430129,"user_tz":-330,"elapsed":6,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[],"source":["# def eval_model(num_classes, model, X_val, y_val, y_cols, X_train, y_train):\n","#     # Compute loss and accuracy using model.evaluate()\n","#     # loss, acc = model.evaluate(X_val, y_val)\n","\n","#     y_pred = model.predict(X_val)\n","#     print('y_pred', y_pred)\n","#     print('y_val', y_val)\n","\n","#     if (num_classes == 3) :\n","\n","#         # Convert y_val to multiclass format\n","#         y_val = np.argmax(y_val, axis=1)\n","\n","#         # Convert y_pred to multiclass format\n","#         y_pred = np.argmax(y_pred, axis=1)\n","\n","#         print('y_pred', y_pred)\n","#         print('y_val', y_val)\n","\n","#         # Check the type of y_pred_probs\n","#         print(\"Type of y_pred using model.predict:\", type(y_pred))\n","#         print(\"shape of the y_pred using model.predict:\", y_pred.shape)\n","\n","#         # Compute confusion matrix\n","#         # y_val_argmax = np.argmax(y_val, axis=1)\n","#         # y_pred_argmax = np.argmax(y_pred, axis=1)\n","#         # conf_mat = confusion_matrix(y_val_argmax, y_pred_argmax)\n","\n","#         conf_mat = mt.confusion_matrix(y_val, y_pred)\n","#         print(\"confusion matrix \", conf_mat)\n","\n","#         target_names = y_cols\n","\n","#         print(\"classification report\", mt.classification_report(y_val, y_pred, target_names=target_names, digits = 3))\n","\n","#         # Compute classification report\n","#         report = mt.classification_report(y_val, y_pred, target_names=target_names, output_dict=True)\n","#         report_df = pd.DataFrame(report).T\n","\n","#         print(\"classification report in dataframe - match accuracy with model.evaluate \")\n","#         print(report_df)\n","\n","#         acc = report_df.iloc[3,1]\n","\n","#         # Select the first three rows\n","#         report_df_top3 = report_df.head(3)\n","\n","#         # Calculate average metrics for the first three rows\n","#         avg_precision = report_df_top3['precision'].mean()\n","#         avg_recall = report_df_top3['recall'].mean()\n","#         avg_f1_score = report_df_top3['f1-score'].mean()\n","\n","#         print(f\"Average Precision (first 3 classes): {avg_precision:.3f}\")\n","#         print(f\"Average Recall (first 3 classes): {avg_recall:.3f}\")\n","#         print(f\"Average F1-Score (first 3 classes): {avg_f1_score:.3f}\")\n","\n","#         ########### Here explicitly calculating f1 for train and test #############\n","\n","#         val_f1 = f1_score(y_val, y_pred, average='weighted')\n","#         print(\"test f1 score\", val_f1)\n","\n","#         tr_f1 = f1_score(y_train, model.predict(X_train), average='weighted')\n","#         print(\"train f1 score\", tr_f1)\n","\n","#         metrics = {\n","#             'train_f1': tr_f1,\n","#             'test_f1': val_f1\n","#         }\n","\n","#     elif (num_classes == 2) :\n","\n","#         # Convert y_val to multiclass format\n","#         y_val = np.argmax(y_val, axis=1)\n","\n","#         # Convert y_pred to multiclass format\n","#         y_pred = np.argmax(y_pred, axis=1)\n","\n","#         print('y_pred', y_pred)\n","#         print('y_val', y_val)\n","\n","#         # Check the type of y_pred_probs\n","#         print(\"Type of y_pred using model.predict:\", type(y_pred))\n","#         print(\"shape of the y_pred using model.predict:\", y_pred.shape)\n","\n","#         # Compute confusion matrix\n","#         conf_mat = confusion_matrix(y_val, y_pred)\n","\n","#         # Compute confusion matrix\n","#         # y_pred = np.argmax(y_pred, axis=1)\n","#         # conf_mat = confusion_matrix(y_val, y_pred)\n","\n","#         # Compute metrics from confusion matrix\n","#         tn, fp, fn, tp = conf_mat.ravel()\n","#         precision = precision_score(y_val, y_pred)\n","#         recall = recall_score(y_val, y_pred)\n","#         f1 = f1_score(y_val, y_pred)\n","#         acc = accuracy_score(y_val, y_pred)\n","\n","#         ########### Here explicitly calculating f1 for train and test #############\n","\n","#         val_f1 = f1_score(y_val, y_pred)\n","#         print(\"test f1 score\", val_f1)\n","\n","#         tr_f1 = f1_score(y_train, model.predict(X_train))\n","#         print(\"train f1 score\", tr_f1)\n","\n","#         metrics = {\n","#             'train_f1': tr_f1,\n","#             'test_f1': val_f1\n","#         }\n","\n","#     else :\n","#         metrics = {}\n","\n","#     return metrics"]},{"cell_type":"code","source":["def eval_model(num_classes, model, X_val, y_val, y_cols, X_train, y_train):\n","    y_pred = model.predict(X_val)\n","    y_pred_train = model.predict(X_train)\n","\n","    print('y_pred shape:', y_pred.shape)\n","    print('y_val shape:', y_val.shape)\n","\n","    # For multi-class problems, y_val and y_pred might already be in the correct format\n","    # If they're one-hot encoded, convert them to class labels\n","    if y_val.ndim == 2 and y_val.shape[1] > 1:\n","        y_val = np.argmax(y_val, axis=1)\n","    if y_pred.ndim == 2 and y_pred.shape[1] > 1:\n","        y_pred = np.argmax(y_pred, axis=1)\n","\n","    print('y_pred:', y_pred)\n","    print('y_val:', y_val)\n","\n","    # Compute confusion matrix\n","    conf_mat = confusion_matrix(y_val, y_pred)\n","    print(\"Confusion matrix:\", conf_mat)\n","\n","    # Compute classification report\n","    target_names = y_cols if num_classes > 2 else None\n","    report = mt.classification_report(y_val, y_pred, target_names=target_names, output_dict=True)\n","    report_df = pd.DataFrame(report).T\n","    print(\"Classification report:\")\n","    print(report_df)\n","\n","    # Compute F1 scores\n","    if num_classes == 2:\n","        val_f1 = f1_score(y_val, y_pred)\n","        tr_f1 = f1_score(y_train, y_pred_train)\n","    else:\n","        val_f1 = f1_score(y_val, y_pred, average='weighted')\n","        tr_f1 = f1_score(y_train, y_pred_train, average='weighted')\n","\n","    print(\"Test F1 score:\", val_f1)\n","    print(\"Train F1 score:\", tr_f1)\n","\n","    # Compute accuracy\n","    acc = accuracy_score(y_val, y_pred)\n","    print(\"Accuracy:\", acc)\n","\n","    metrics = {\n","        'train_f1': tr_f1,\n","        'test_f1': val_f1\n","        # 'accuracy': acc,\n","        # 'confusion_matrix': conf_mat.tolist(),  # Convert to list for easy JSON serialization\n","    }\n","\n","    return metrics"],"metadata":{"id":"SqwvY8Bp84gP","executionInfo":{"status":"ok","timestamp":1722482531593,"user_tz":-330,"elapsed":4,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","execution_count":46,"metadata":{"id":"XNvEBOc_IUEv","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1722482430130,"user_tz":-330,"elapsed":6,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}},"outputId":"349b1656-77d7-4ca6-9cb5-7c282cb404a4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\n# Replacing nan values to 0\\ndef nan_to_0(data):\\n\\n    df1 = data.copy()\\n\\n    for idx, row in df1.iterrows():\\n        arr = row['psi_matrix']\\n        matrix = np.nan_to_num(arr, copy = True, nan = 0.0)\\n        df1.at[idx, 'psi_matrix'] = matrix\\n\\n    print(df1)\\n\\n    return df1\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":46}],"source":["'''\n","# Replacing nan values to 0\n","def nan_to_0(data):\n","\n","    df1 = data.copy()\n","\n","    for idx, row in df1.iterrows():\n","        arr = row['psi_matrix']\n","        matrix = np.nan_to_num(arr, copy = True, nan = 0.0)\n","        df1.at[idx, 'psi_matrix'] = matrix\n","\n","    print(df1)\n","\n","    return df1\n","'''"]},{"cell_type":"code","execution_count":47,"metadata":{"collapsed":true,"id":"29gD-W9Xidwo","outputId":"0c12cc11-af91-44b3-99ed-8a9a0cc64c25","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1722482430952,"user_tz":-330,"elapsed":8,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef computeMinMax(X):\\n    min_matrix = X.min(axis = 0)\\n    max_matrix = X.max(axis = 0)\\n    return (min_matrix, max_matrix)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":47}],"source":["'''\n","def computeMinMax(X):\n","    min_matrix = X.min(axis = 0)\n","    max_matrix = X.max(axis = 0)\n","    return (min_matrix, max_matrix)\n","'''"]},{"cell_type":"code","execution_count":48,"metadata":{"collapsed":true,"id":"VPaen_EqiduJ","outputId":"2c4a870e-bb4c-4a7b-ddd4-414a53f8063f","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1722482430953,"user_tz":-330,"elapsed":8,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef normalize_instance(X, minn, maxx):\\n    normalised_X = np.zeros(shape=(X.shape[0], X.shape[1]))\\n\\n    for idx, x in np.ndenumerate(X):\\n        if minn[idx] == maxx[idx]:\\n            normalised_X[idx] = x\\n        else:\\n            normalised_X[idx] = (x - minn[idx])/(maxx[idx] - minn[idx])\\n    return normalised_X\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":48}],"source":["'''\n","def normalize_instance(X, minn, maxx):\n","    normalised_X = np.zeros(shape=(X.shape[0], X.shape[1]))\n","\n","    for idx, x in np.ndenumerate(X):\n","        if minn[idx] == maxx[idx]:\n","            normalised_X[idx] = x\n","        else:\n","            normalised_X[idx] = (x - minn[idx])/(maxx[idx] - minn[idx])\n","    return normalised_X\n","'''"]},{"cell_type":"code","execution_count":49,"metadata":{"collapsed":true,"id":"IpA8FXAZjp64","outputId":"90c9fd5a-c140-4f9b-9f2b-38303c3631d5","colab":{"base_uri":"https://localhost:8080/","height":140},"executionInfo":{"status":"ok","timestamp":1722482430953,"user_tz":-330,"elapsed":7,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ndef normalize(X_train, X_val):\\n    # Assuming X_train is your DataFrame with matrices in a single column\\n    matrices = X_train  # Get the values from the \\'matrices\\' column\\n    # Convert the matrices to a 2D NumPy array\\n    X_train_2d = np.stack(matrices)\\n\\n    # Assuming X_train is your DataFrame with matrices in a single column\\n    matrices = X_val  # Get the values from the \\'matrices\\' column\\n    # Convert the matrices to a 2D NumPy array\\n    X_val_2d = np.stack(matrices)\\n\\n    min_matrix, max_matrix = computeMinMax(X_train_2d)\\n\\n    print(\"shape of min matrix\", min_matrix.shape)\\n    print(\"shape of max matrix\", max_matrix.shape)\\n\\n    normalized_instances = []\\n    for instance in X_train_2d:\\n        normalized_instance = normalize_instance(instance, min_matrix, max_matrix)\\n        normalized_instances.append(normalized_instance)\\n\\n    # Convert the list of normalized instances to a NumPy array\\n    X_normalized_trained_2d = np.array(normalized_instances)\\n\\n    normalized_instances = []\\n    for instance in X_val_2d:\\n        normalized_instance = normalize_instance(instance, min_matrix, max_matrix)\\n        normalized_instances.append(normalized_instance)\\n\\n    # Convert the list of normalized instances to a NumPy array\\n    X_normalized_val_2d = np.array(normalized_instances)\\n\\n    return (X_normalized_trained_2d, X_normalized_val_2d)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":49}],"source":["'''\n","def normalize(X_train, X_val):\n","    # Assuming X_train is your DataFrame with matrices in a single column\n","    matrices = X_train  # Get the values from the 'matrices' column\n","    # Convert the matrices to a 2D NumPy array\n","    X_train_2d = np.stack(matrices)\n","\n","    # Assuming X_train is your DataFrame with matrices in a single column\n","    matrices = X_val  # Get the values from the 'matrices' column\n","    # Convert the matrices to a 2D NumPy array\n","    X_val_2d = np.stack(matrices)\n","\n","    min_matrix, max_matrix = computeMinMax(X_train_2d)\n","\n","    print(\"shape of min matrix\", min_matrix.shape)\n","    print(\"shape of max matrix\", max_matrix.shape)\n","\n","    normalized_instances = []\n","    for instance in X_train_2d:\n","        normalized_instance = normalize_instance(instance, min_matrix, max_matrix)\n","        normalized_instances.append(normalized_instance)\n","\n","    # Convert the list of normalized instances to a NumPy array\n","    X_normalized_trained_2d = np.array(normalized_instances)\n","\n","    normalized_instances = []\n","    for instance in X_val_2d:\n","        normalized_instance = normalize_instance(instance, min_matrix, max_matrix)\n","        normalized_instances.append(normalized_instance)\n","\n","    # Convert the list of normalized instances to a NumPy array\n","    X_normalized_val_2d = np.array(normalized_instances)\n","\n","    return (X_normalized_trained_2d, X_normalized_val_2d)\n","'''"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"0JamuydxczFl","executionInfo":{"status":"ok","timestamp":1722482431786,"user_tz":-330,"elapsed":5,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[],"source":["def set_threshold(df):\n","    thresholds = []  # Initialize an empty list to store thresholds\n","\n","    # Iterate over each row in the DataFrame\n","    for index, row in df.iterrows():\n","        # Extract the matrix from the attribute of the current row\n","        matrix = row['psi_matrix']\n","\n","        # Flatten the matrix into a 1D array\n","        flat_matrix = matrix.flatten()\n","\n","        # Create a histogram of the values in the array\n","        hist, bins = np.histogram(flat_matrix, bins=5)  # Adjust the number of bins as needed\n","\n","        # Find the bin with the highest count\n","        max_count_index = np.argmax(hist)\n","\n","        # Determine the corresponding value (bin edge) as the threshold\n","        threshold_value = bins[max_count_index + 1]\n","\n","        # Print the threshold value for the current row (optional)\n","        print(\"Threshold value for row\", index, \":\", threshold_value)\n","\n","        # Append the threshold value to the list\n","        thresholds.append(threshold_value)\n","\n","    # Calculate the mean of the thresholds\n","    mean_threshold = np.mean(thresholds)\n","\n","    return mean_threshold"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"JxqFryyyd8aR","outputId":"0de34f91-ac4d-4d7e-c1c9-9b57435d957b","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1722482431786,"user_tz":-330,"elapsed":4,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\ndef binarize_matrices(df, threshold):\\n\\n    def binarize_matrix(matrix):\\n        binarized_matrix = np.copy(matrix)\\n        # Apply thresholding to the matrix\\n        binarised_matrix = np.where(matrix >= threshold, 1, 0)\\n        return binarized_matrix\\n\\n    # Make a copy of the original DataFrame\\n    final_df = df.copy()\\n\\n    # Iterate over each row and update the 'psi_matrix' column\\n    for index, row in final_df.iterrows():\\n        matrix = row['psi_matrix']\\n        binarized_matrix = binarize_matrix(matrix)\\n        # Update the matrix attribute in the copied DataFrame\\n        final_df.at[index, 'psi_matrix'] = binarized_matrix\\n\\n    return final_df\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":51}],"source":["'''\n","def binarize_matrices(df, threshold):\n","\n","    def binarize_matrix(matrix):\n","        binarized_matrix = np.copy(matrix)\n","        # Apply thresholding to the matrix\n","        binarised_matrix = np.where(matrix >= threshold, 1, 0)\n","        return binarized_matrix\n","\n","    # Make a copy of the original DataFrame\n","    final_df = df.copy()\n","\n","    # Iterate over each row and update the 'psi_matrix' column\n","    for index, row in final_df.iterrows():\n","        matrix = row['psi_matrix']\n","        binarized_matrix = binarize_matrix(matrix)\n","        # Update the matrix attribute in the copied DataFrame\n","        final_df.at[index, 'psi_matrix'] = binarized_matrix\n","\n","    return final_df\n","'''"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"L0rhgu86t62I","executionInfo":{"status":"ok","timestamp":1722482431786,"user_tz":-330,"elapsed":3,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[],"source":["def binarize_matrices(df, threshold):\n","    # Make a copy of the original DataFrame\n","    final_df = df.copy()\n","\n","    # Iterate over each row and update the 'psi_matrix' column\n","    for index, row in final_df.iterrows():\n","        matrix = row['psi_matrix']\n","        newmatrix = np.where(matrix>=threshold, 1, 0)\n","        # Update the matrix attribute in the copied DataFrame\n","        final_df.at[index, 'psi_matrix'] = newmatrix\n","\n","    return final_df"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"3Jdn4ofCB2CI","executionInfo":{"status":"ok","timestamp":1722482432640,"user_tz":-330,"elapsed":4,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[],"source":["def remove_loop(df):\n","\n","    final_df = df.copy()\n","\n","    # Define the identity matrix\n","    identity_matrix = np.eye(132)\n","\n","    # Iterate over each row\n","    for i, row in final_df.iterrows():\n","        # Check if the first column contains a NumPy array\n","        if isinstance(row['psi_matrix'], np.ndarray):\n","          # Subtract the identity matrix from the NumPy array\n","            final_df.at[i, 'psi_matrix'] = row['psi_matrix'] - identity_matrix\n","        else:\n","            # Skip this row if the first column doesn't contain a NumPy array\n","            print(f\"Skipping row {i}: First column doesn't contain a NumPy array.\")\n","\n","    return final_df"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"wuwTZiUOB1-z","executionInfo":{"status":"ok","timestamp":1722482432640,"user_tz":-330,"elapsed":3,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[],"source":["def extract_features(df):\n","    # Initialize lists to store the calculated features\n","    clustering_coefficients = []\n","    average_node_degrees = []\n","    global_efficiencies = []\n","    characteristic_path_lengths = []\n","    assortativity = []\n","\n","    # Iterate over each row in the input DataFrame\n","    for index, row in df.iterrows():\n","        # Step 1: Extract the thresholded adjacency matrix\n","        adjacency_matrix = row['psi_matrix']\n","\n","        # Step 2: Convert the adjacency matrix to a NetworkX graph\n","        G = nx.from_numpy_array(adjacency_matrix)\n","\n","        # Step 3: Calculate the topological features\n","        clustering_coefficient = nx.average_clustering(G)\n","        average_node_degree = sum(dict(G.degree()).values()) / len(G)\n","        global_efficiency = nx.global_efficiency(G)\n","        try:\n","            characteristic_path_length = nx.average_shortest_path_length(G)\n","        except nx.NetworkXError:\n","            characteristic_path_length = 0\n","\n","        assort = nx.degree_assortativity_coefficient(G)\n","\n","        # Step 4: Append the calculated features to the lists\n","        clustering_coefficients.append(clustering_coefficient)\n","        average_node_degrees.append(average_node_degree)\n","        global_efficiencies.append(global_efficiency)\n","        characteristic_path_lengths.append(characteristic_path_length)\n","        assortativity.append(assort)\n","\n","    # Create a new DataFrame with the calculated features and original columns\n","    topological_features_df = pd.DataFrame({\n","        'subject': df['subject'],\n","        'clustering_coefficient': clustering_coefficients,\n","        'average_node_degree': average_node_degrees,\n","        'global_efficiency': global_efficiencies,\n","        'characteristic_path_length': characteristic_path_lengths,\n","        'assortativity': assortativity,\n","        'autism': df['autism'],\n","        'adhd': df['adhd'],\n","        'healthy': df['healthy']\n","    })\n","\n","    # Print the DataFrame with calculated features\n","    print(topological_features_df)\n","    topological_features_df.fillna(0, inplace=True)\n","    # topological_features_df.to_csv('/content/drive/MyDrive/Colab Notebooks/ROIxTimeseries/psi_features_data.csv', index=False)\n","\n","    return topological_features_df"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"WmJP3YLbjOca","executionInfo":{"status":"ok","timestamp":1722482432640,"user_tz":-330,"elapsed":3,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[],"source":["def make_dataset(choice):\n","    # Load dataframe from the pickle file\n","    data = pd.read_pickle(r\"/content/drive/MyDrive/Colab Notebooks/ROIxTimeseries/psi_data.pkl\")\n","\n","    if choice == 'A':\n","        # Filter rows where 'adhd' or 'autism' is 1 (keep only ADHD or autism subjects)\n","        data = data[(data['adhd'] == 1) | (data['autism'] == 1)]\n","        y_cols = ['adhd', 'autism']  # Specify the columns for y\n","    elif choice == 'B':\n","        # Filter rows where 'autism' or 'healthy' is 1 (keep only autism or healthy subjects)\n","        data = data[(data['autism'] == 1) | (data['healthy'] == 1)]\n","        y_cols = ['autism', 'healthy']  # Specify the columns for y\n","    elif choice == 'C':\n","        # Filter rows where 'adhd' or 'healthy' is 1 (keep only ADHD or healthy subjects)\n","        data = data[(data['adhd'] == 1) | (data['healthy'] == 1)]\n","        y_cols = ['adhd', 'healthy']  # Specify the columns for y\n","    elif choice == 'D':\n","        # Keep all rows\n","        y_cols = ['adhd', 'autism', 'healthy']  # Specify the columns for y\n","    else:\n","        print(\"Invalid choice. Please enter 'A', 'B', 'C', or 'D'.\")\n","        return pd.DataFrame(), []\n","\n","    print(data)\n","\n","    # df1 = nan_to_0(data)\n","\n","    print(y_cols)\n","    return data, y_cols"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"M3mrlUZmjOaS","executionInfo":{"status":"ok","timestamp":1722482433313,"user_tz":-330,"elapsed":4,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}}},"outputs":[],"source":["def driver(choice):\n","\n","    # choice = input(\"Enter your choice (A, B, C, or D): \").upper()\n","\n","    choice = choice.upper()\n","\n","    df, y_cols = make_dataset(choice)\n","    #print(d.head)\n","\n","    # avg_thresh = set_threshold(df)\n","    # print(\"Mean Threshold\", avg_thresh)\n","    thresh = 0.3\n","\n","    df = binarize_matrices(df, thresh)\n","    df_loop = remove_loop(df)\n","    f_df = extract_features(df_loop)\n","\n","    X = f_df.drop(columns=['subject','autism','adhd','healthy'])\n","    print(X.isna().sum())\n","\n","    y = f_df[y_cols].values\n","    #y = to_categorical(y, num_classes=3)\n","    # print(y.shape)\n","    # print(y)\n","    print(\"type of label columns\", type(y))\n","\n","    # Get the number of classes\n","    num_classes = y.shape[1]\n","    print(\"No. of classes\", num_classes)\n","\n","    # input_shape = X[0].shape\n","    # print(\"Input_shape:\", input_shape)\n","\n","    result_df = pd.DataFrame(columns = ['seed','fold','train_f1','test_f1', 'best_params'])\n","\n","    # if (num_classes == 2) :\n","    #     result_df = pd.DataFrame(columns = ['seed','fold','train_f1','test_f1', 'best_params'])\n","    # elif (num_classes == 3) :\n","    #     result_df = pd.DataFrame(columns = ['seed','fold','train_f1','test_f1', 'best_params'])\n","    # else :\n","    #     result_df = {}\n","\n","    # Set a fixed seed for reproducibility\n","    np.random.seed(28)\n","\n","    # Manually set random seeds\n","    random_seeds = np.random.randint(0, 50, size=2)\n","    print(\"Random seeds for outer loops:\", random_seeds)\n","\n","    all_results = []\n","\n","    for outer_loop, random_seed in enumerate(random_seeds):\n","        print(f\"Outer loop iteration: {outer_loop + 1}, Random seed: {random_seed}\")\n","\n","        kf = KFold(n_splits=3, shuffle=True, random_state=random_seed)\n","\n","        for i, (train_index, val_index) in enumerate(kf.split(X, y)):\n","\n","            print(\"FOLD : \", i+1)\n","\n","            X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n","            y_train, y_val = y[train_index], y[val_index]\n","\n","            grid_search = get_model(num_classes)\n","            grid_search.fit(X_train, np.array(y_train))\n","            best_model = grid_search.best_estimator_\n","\n","            # Get the best parameters\n","            best_params = grid_search.best_params_\n","\n","            trained_m = compile_fit(best_model, X_train, np.array(y_train))\n","\n","            # Calculate train and test F1 scores\n","            # train_f1 = grid_search.score(X_train, np.array(y_train))\n","            # test_f1 = grid_search.score(X_val, np.array(y_val))\n","\n","            res = eval_model(num_classes, trained_m, X_val, y_val, y_cols, X_train, np.array(y_train))\n","            # Store the results\n","            res['seed']= random_seed\n","            res['fold']= i+1\n","            res['best_params']= best_params\n","            res = pd.DataFrame([res])\n","            result_df = pd.concat([result_df,res], ignore_index=True)\n","\n","        all_results.append(result_df)\n","        print(f\"Outer loop {outer_loop + 1} result_df:\")\n","        print(result_df)\n","        print(\"\\n\")\n","\n","            # compiled_m = get_model(num_classes)\n","\n","            # trained_m = compile_fit(compiled_m, X_train, np.array(y_train))\n","            # #plot_history(history, i+1)\n","\n","            # scores = eval_model(num_classes, trained_m, X_val, y_val, y_cols)\n","            # scores['fold']=i+1\n","            # print(\"Scores\", scores)\n","            # scores = pd.DataFrame([scores])\n","            # result_df = pd.concat([result_df,scores], ignore_index=True)\n","\n","    return all_results"]},{"cell_type":"code","source":["# Define a list of choices\n","# choices = ['A', 'B', 'C', 'D']\n","choices = ['A', 'B', 'C']\n","\n","# Create an empty dictionary to store the result dataframes\n","result_dfs = {}\n","\n","# Loop through each choice\n","for choice in choices:\n","    # Call the driver() function with the current choice\n","    result_df = driver(choice)\n","\n","    # Store the result dataframe in the dictionary with the choice as the key\n","    result_dfs[choice] = result_df\n","\n","    combined_df = result_df[-1]\n","\n","    # Save the combined results to a CSV file\n","    filename = f\"choice_{choice}_results.csv\"\n","    combined_df.to_csv(filename, index=False)\n","    print(f\"Saved combined results for choice {choice} to {filename}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"WNErhFPSsqP3","executionInfo":{"status":"error","timestamp":1722482581577,"user_tz":-330,"elapsed":39530,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}},"outputId":"bf037cee-835e-43da-f770-094920d0314d"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["        subject                                         psi_matrix  adhd  \\\n","0      subject1  [[1.0, 0.8016814987579317, 0.839641852247262, ...     0   \n","1      subject2  [[1.0, 0.9101210641534417, 0.576123749895304, ...     0   \n","2      subject3  [[1.0, 0.7530150385517623, 0.5067414702685676,...     0   \n","3      subject4  [[1.0, 0.817812140965262, 0.520061068095793, 0...     0   \n","4      subject5  [[1.0, 0.7477157295145737, 0.8259250654415865,...     0   \n","..          ...                                                ...   ...   \n","105  subject106  [[1.0, 0.7190486619620364, 0.3595353948075287,...     1   \n","106  subject107  [[1.0, 0.892350228080446, 0.6115053964108019, ...     1   \n","107  subject108  [[1.0, 0.9383744300499544, 0.7480842785117325,...     1   \n","108  subject109  [[1.0, 0.9300675513188588, 0.9240553408215981,...     1   \n","109  subject110  [[1.0, 0.7946738858174632, 0.5573150422007396,...     1   \n","\n","     autism  healthy  \n","0         1        0  \n","1         1        0  \n","2         1        0  \n","3         1        0  \n","4         1        0  \n","..      ...      ...  \n","105       0        0  \n","106       0        0  \n","107       0        0  \n","108       0        0  \n","109       0        0  \n","\n","[110 rows x 5 columns]\n","['adhd', 'autism']\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/networkx/algorithms/assortativity/correlation.py:302: RuntimeWarning: invalid value encountered in scalar divide\n","  return float((xy * (M - ab)).sum() / np.sqrt(vara * varb))\n"]},{"output_type":"stream","name":"stdout","text":["        subject  clustering_coefficient  average_node_degree  \\\n","0      subject1                0.979236           125.666667   \n","1      subject2                0.803892            83.863636   \n","2      subject3                0.915260           111.863636   \n","3      subject4                0.860111            98.878788   \n","4      subject5                0.942771           117.348485   \n","..          ...                     ...                  ...   \n","105  subject106                0.869131            98.363636   \n","106  subject107                0.981212           123.833333   \n","107  subject108                0.931577           109.515152   \n","108  subject109                0.996633           130.348485   \n","109  subject110                0.904062           101.621212   \n","\n","     global_efficiency  characteristic_path_length  assortativity  autism  \\\n","0             0.979644                    1.040712      -0.040772       1   \n","1             0.820071                    1.359935       0.260843       1   \n","2             0.926960                    1.146079      -0.003003       1   \n","3             0.877381                    1.245316       0.021496       1   \n","4             0.947895                    1.104210       0.005417       1   \n","..                 ...                         ...            ...     ...   \n","105           0.873622                    1.260005       0.020239       0   \n","106           0.972608                    1.054939      -0.021389       0   \n","107           0.910151                    0.000000       0.038718       0   \n","108           0.997513                    1.004973      -0.020228       0   \n","109           0.887559                    1.226116       0.033754       0   \n","\n","     adhd  healthy  \n","0       0        0  \n","1       0        0  \n","2       0        0  \n","3       0        0  \n","4       0        0  \n","..    ...      ...  \n","105     1        0  \n","106     1        0  \n","107     1        0  \n","108     1        0  \n","109     1        0  \n","\n","[110 rows x 9 columns]\n","clustering_coefficient        0\n","average_node_degree           0\n","global_efficiency             0\n","characteristic_path_length    0\n","assortativity                 0\n","dtype: int64\n","type of label columns <class 'numpy.ndarray'>\n","No. of classes 2\n","Random seeds for outer loops: [1 5]\n","Outer loop iteration: 1, Random seed: 1\n","FOLD :  1\n","y_pred shape: (37, 2)\n","y_val shape: (37, 2)\n","y_pred: [1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0]\n","y_val: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","Confusion matrix: [[19  3]\n"," [ 5 10]]\n","Classification report:\n","              precision    recall  f1-score    support\n","0              0.791667  0.863636  0.826087  22.000000\n","1              0.769231  0.666667  0.714286  15.000000\n","accuracy       0.783784  0.783784  0.783784   0.783784\n","macro avg      0.780449  0.765152  0.770186  37.000000\n","weighted avg   0.782571  0.783784  0.780762  37.000000\n"]},{"output_type":"error","ename":"ValueError","evalue":"Target is multilabel-indicator but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted', 'samples'].","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-59-fe399df3b2f1>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchoice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Call the driver() function with the current choice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mresult_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Store the result dataframe in the dictionary with the choice as the key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-56-4bf3bf11f1c4>\u001b[0m in \u001b[0;36mdriver\u001b[0;34m(choice)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;31m# test_f1 = grid_search.score(X_val, np.array(y_val))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;31m# Store the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-58-818ecea96689>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(num_classes, model, X_val, y_val, y_cols, X_train, y_train)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mval_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mtr_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mval_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m                     )\n\u001b[1;32m    213\u001b[0m                 ):\n\u001b[0;32m--> 214\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.66666667\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m        \u001b[0;34m,\u001b[0m \u001b[0;36m0.66666667\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \"\"\"\n\u001b[0;32m-> 1239\u001b[0;31m     return fbeta_score(\n\u001b[0m\u001b[1;32m   1240\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1411\u001b[0m     \"\"\"\n\u001b[1;32m   1412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m     _, _, f, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[1;32m   1414\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mglobal_skip_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skip_parameter_validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mfunc_sig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1722\u001b[0m     \"\"\"\n\u001b[1;32m   1723\u001b[0m     \u001b[0mzero_division_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_zero_division\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m                 \u001b[0maverage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1519\u001b[0m                 \u001b[0;34m\"Target is %s but average='binary'. Please \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m                 \u001b[0;34m\"choose another average setting, one of %r.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Target is multilabel-indicator but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted', 'samples']."]}]},{"cell_type":"markdown","metadata":{"id":"N6BGNpdat62J"},"source":["### CHANGES MADE:\n","\n","1. Binariztion method corrected\n","2. clustering coefficient set to 0 in case of disconnected graph\n","3. assortativity set to zero where it was NaN"]},{"cell_type":"code","source":[],"metadata":{"id":"z_5wKwzssqMg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lyB_mMKBsqKH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yAI5Hf-_sqHV"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mly2BAiCrrd1","outputId":"7194f5f7-3c3d-404e-d87f-5e82b3b2a117","scrolled":false,"executionInfo":{"status":"ok","timestamp":1718175985395,"user_tz":-330,"elapsed":53591,"user":{"displayName":"Karamjot","userId":"14290690433704537570"}},"collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["        subject                                         psi_matrix  adhd  \\\n","0      subject1  [[1.0, 0.8016814987579317, 0.839641852247262, ...     0   \n","1      subject2  [[1.0, 0.9101210641534417, 0.576123749895304, ...     0   \n","2      subject3  [[1.0, 0.7530150385517623, 0.5067414702685676,...     0   \n","3      subject4  [[1.0, 0.817812140965262, 0.520061068095793, 0...     0   \n","4      subject5  [[1.0, 0.7477157295145737, 0.8259250654415865,...     0   \n","..          ...                                                ...   ...   \n","160  subject161  [[1.0, 0.8966881181531998, 0.7033025053563576,...     0   \n","161  subject162  [[1.0, 0.4594896330132013, 0.08037697590578775...     0   \n","162  subject163  [[1.0, 0.9030386042889056, 0.7605824095393477,...     0   \n","163  subject164  [[1.0, 0.8822326646328201, 0.6885390808656763,...     0   \n","164  subject165  [[1.0, 0.9726522672232811, 0.8350244047495731,...     0   \n","\n","     autism  healthy  \n","0         1        0  \n","1         1        0  \n","2         1        0  \n","3         1        0  \n","4         1        0  \n","..      ...      ...  \n","160       0        1  \n","161       0        1  \n","162       0        1  \n","163       0        1  \n","164       0        1  \n","\n","[165 rows x 5 columns]\n","['adhd', 'autism', 'healthy']\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/networkx/algorithms/assortativity/correlation.py:302: RuntimeWarning: invalid value encountered in scalar divide\n","  return float((xy * (M - ab)).sum() / np.sqrt(vara * varb))\n"]},{"output_type":"stream","name":"stdout","text":["        subject  clustering_coefficient  average_node_degree  \\\n","0      subject1                0.979236           125.666667   \n","1      subject2                0.803892            83.863636   \n","2      subject3                0.915260           111.863636   \n","3      subject4                0.860111            98.878788   \n","4      subject5                0.942771           117.348485   \n","..          ...                     ...                  ...   \n","160  subject161                0.959946           117.303030   \n","161  subject162                0.821577            82.333333   \n","162  subject163                0.952761           116.878788   \n","163  subject164                0.964880           118.742424   \n","164  subject165                0.996221           129.439394   \n","\n","     global_efficiency  characteristic_path_length  assortativity  autism  \\\n","0             0.979644                    1.040712      -0.040772       1   \n","1             0.820071                    1.359935       0.260843       1   \n","2             0.926960                    1.146079      -0.003003       1   \n","3             0.877381                    1.245316       0.021496       1   \n","4             0.947895                    1.104210       0.005417       1   \n","..                 ...                         ...            ...     ...   \n","160           0.947509                    1.105829       0.033160       0   \n","161           0.809951                    1.399607       0.226860       0   \n","162           0.946102                    1.107796      -0.004367       0   \n","163           0.945582                    0.000000       0.030564       0   \n","164           0.994043                    1.011913      -0.019014       0   \n","\n","     adhd  healthy  \n","0       0        0  \n","1       0        0  \n","2       0        0  \n","3       0        0  \n","4       0        0  \n","..    ...      ...  \n","160     0        1  \n","161     0        1  \n","162     0        1  \n","163     0        1  \n","164     0        1  \n","\n","[165 rows x 9 columns]\n"]}],"source":["# Define a list of choices\n","# choices = ['A', 'B', 'C', 'D']\n","choices = ['D']\n","\n","# Create an empty dictionary to store the result dataframes\n","result_dfs = {}\n","\n","# Loop through each choice\n","for choice in choices:\n","    # Call the driver() function with the current choice\n","    result_df = driver(choice)\n","\n","    # Store the result dataframe in the dictionary with the choice as the key\n","    result_dfs[choice] = result_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TiBtoUTLr1W2"},"outputs":[],"source":["print(result_dfs['A'])\n","result_dfs['A'].to_csv('results/PSI_RF-globalfeatures/adhd-autism.csv', mode = 'w', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"in8Zi0MBr6bV"},"outputs":[],"source":["print(result_dfs['B'])\n","result_dfs['B'].to_csv('results/PSI_RF-globalfeatures/autism-healthy.csv', mode = 'w', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"aUvQ70JWr6RV"},"outputs":[],"source":["print(result_dfs['C'])\n","result_dfs['C'].to_csv('results/PSI_RF-globalfeatures/adhd-healthy.csv', mode = 'w', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"apjP_FSAr6Oe"},"outputs":[],"source":["print(result_dfs['D'])\n","result_dfs['D'].to_csv('results/PSI_RF-globalfeatures/adhd-autism-healthy.csv', mode = 'w', index=False)"]}],"metadata":{"colab":{"provenance":[{"file_id":"10Yf3sHyS6b8bsO_4U-VJQjR1Ke3oOLDK","timestamp":1722477610981}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}